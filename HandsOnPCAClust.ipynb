{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711be780",
   "metadata": {},
   "source": [
    "> **Note:** Run this notebook from the cloned `SummerSchool` repository using the `SummerSchool` conda environment described in the [README](README.md).\n",
    "\n",
    "# Dimensionality Reduction & Clustering for Calcium Imaging\n",
    "\n",
    "Modern calcium imaging experiments record population activity from hundreds of neurons at millisecond resolution. The raw data live in a very high-dimensional space where each neuron contributes one axis, and it is sometimes difficult to reason about population dynamics or coordinated patterns directly from the raw traces. In this notebook we will follow a practical workflow that reduces the dimensionality of the data, discovers structure through clustering, and interprets the resulting groups in neuroscientifically meaningful terms.\n",
    "\n",
    "Along the way we will emphasize not just *how* to run the code, but *why* each step matters. Dimensionality reduction provides low-dimensional views that distill population activity into latent factors, while clustering reveals candidate functional ensembles. Both are indispensable tools when bridging raw fluorescence traces and hypotheses about circuit organization.\n",
    "\n",
    "## Learning objectives\n",
    "1. Motivate dimensionality reduction for neural population data and understand what information it preserves or discards.\n",
    "2. Compare linear methods such as PCA with non-linear manifold learning techniques (t-SNE, UMAP) and know when to reach for each.\n",
    "3. Apply correlation- and embedding-based clustering strategies to organize neurons into putative functional groups.\n",
    "4. Evaluate cluster stability and biological plausibility so that downstream interpretations rest on solid evidence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0093269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction & Clustering for Calcium Imaging\n",
    "# Simplified Lecture Version\n",
    "\n",
    "# --- Standard Library ---\n",
    "import warnings\n",
    "\n",
    "# --- Scientific Libraries ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import loadmat\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\", \"Times New Roman\", \"DejaVu Serif\"],\n",
    "    \"text.usetex\": False,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"axes.linewidth\": 0.5,\n",
    "    \"lines.linewidth\": 0.5,\n",
    "})\n",
    "sns.set_theme(style=\"white\", context=\"talk\", palette=\"Set2\")\n",
    "\n",
    "print(\"=== DIMENSIONALITY REDUCTION & CLUSTERING FOR CALCIUM IMAGING ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bb1b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. INTRODUCTION & DATA LOADING\n",
    "\n",
    "### Why start with dimensionality reduction?\n",
    "Calcium imaging datasets are matrices of shape `neurons × time`. Each neuron defines a dimension, so even a modest recording of 500 cells lives in a 500-D space. Visualizing or reasoning about such spaces directly is not possible, and the curse of dimensionality makes distances noisy and unintuitive. Dimensionality reduction seeks a low-dimensional latent space that captures the dominant co-activation patterns, providing both intuition and a staging ground for downstream analyses.\n",
    "\n",
    "### Workflow for this notebook\n",
    "1. **Load and align the data** – bring neural activity and metadata into a tidy format, handling units and time axes carefully.\n",
    "2. **Preprocess** – detrend, normalize, or smooth the traces to reduce measurement artifacts that could dominate the variance.\n",
    "3. **Reduce dimensionality** – extract latent axes that summarize the dominant shared dynamics across neurons.\n",
    "4. **Cluster** – group neurons with similar signatures to propose functional assemblies or cell-type groupings.\n",
    "5. **Interpret** – inspect the latent spaces and cluster assignments to generate hypotheses about circuit organization.\n",
    "\n",
    "Each step feeds the next: poor preprocessing contaminates PCA, and noisy embeddings derail clustering. Treat the pipeline holistically rather than a sequence of independent buttons to press.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73283dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "print(\"1. Loading Sample Data...\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Selector for choosing the dataset\n",
    "dataset_choice = \"population\"  # Change to \"dendrites\" to use the other dataset\n",
    "\n",
    "if dataset_choice == \"dendrites\":\n",
    "    # Load the .mat file for the tree dataset. these were variables directly stored in the .mat file\n",
    "    mat_path = \"Data/M1_dendritic_tree_data.mat\"  # Update this path to the location of your .mat file\n",
    "    mat_data = loadmat(mat_path)\n",
    "\n",
    "    # Extract the variables\n",
    "    behaviour = mat_data['behaviour'].squeeze()  # Squeeze to remove single-dimensional entries\n",
    "    result = mat_data['result']\n",
    "    tax = np.squeeze(mat_data['tax'].T)\n",
    "    coords = mat_data['coords']\n",
    "\n",
    "elif dataset_choice == \"population\":\n",
    "    # Load the .mat file for the M1 dataset. # these were variables stored in a structure in the .mat file\n",
    "    mat_path = \"Data/M1_population_data.mat\"  # Data exported from matlab\n",
    "    mat_data = loadmat(mat_path)\n",
    "\n",
    "    # Extract the variables\n",
    "    mat_data = mat_data['mat_data']\n",
    "    behaviour = mat_data['behaviour'][0, 0].squeeze()  # Indexing to access the nested structure and squeeze to remove single-dimensional entries\n",
    "    result = mat_data['result'][0, 0]  # Indexing to access the nested structure\n",
    "    tax = mat_data['tax'][0, 0].squeeze()  # Indexing to access the nested structure and squeeze to remove single-dimensional entries\n",
    "\n",
    "# Get the dimensions\n",
    "T, N = len(behaviour), result.shape[0]\n",
    "\n",
    "# Ensure result shape is (Individuals, Timepoints)\n",
    "print(result.shape) # N x T\n",
    "print(tax.shape) # T x 1\n",
    "print(behaviour.shape) # T x 1\n",
    "\n",
    "# Align with existing variable names in this notebook\n",
    "behavior = behaviour\n",
    "calcium_data = result\n",
    "time_axis = tax\n",
    "\n",
    "# Replace non-finite values with NaN and remove invalid entries\n",
    "calcium_data = np.where(np.isfinite(calcium_data), calcium_data, np.nan)\n",
    "behavior = np.where(np.isfinite(behavior), behavior, np.nan)\n",
    "initial_n_neurons, initial_n_timepoints = calcium_data.shape\n",
    "valid_neurons = ~np.all(np.isnan(calcium_data), axis=1)\n",
    "calcium_data = calcium_data[valid_neurons]\n",
    "removed_neurons = initial_n_neurons - calcium_data.shape[0]\n",
    "\n",
    "valid_timepoints = ~np.isnan(behavior)\n",
    "valid_timepoints &= ~np.any(np.isnan(calcium_data), axis=0)\n",
    "calcium_data = calcium_data[:, valid_timepoints]\n",
    "behavior = behavior[valid_timepoints]\n",
    "time_axis = time_axis[valid_timepoints]\n",
    "removed_timepoints = initial_n_timepoints - calcium_data.shape[1]\n",
    "\n",
    "n_neurons, n_timepoints = calcium_data.shape\n",
    "print(f\"Removed {removed_neurons} neurons and {removed_timepoints} timepoints with NaN\")\n",
    "print(f\"Dataset cleaned: {n_neurons} neurons, {n_timepoints} timepoints\")\n",
    "\n",
    "assert not np.isnan(calcium_data).any()\n",
    "assert not np.isnan(behavior).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b177c",
   "metadata": {},
   "source": [
    "# 2. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "\n",
    "### Conceptual overview\n",
    "Principal Component Analysis (PCA) finds orthogonal directions (principal components) that capture maximal variance in the data. Projecting the neural activity matrix onto the first few components yields a low-dimensional representation that preserves global covariance structure. Because PCA is linear and deterministic, it is fast, interpretable, and often the first lens through which we inspect high-dimensional recordings.\n",
    "\n",
    "### Why PCA helps with calcium imaging\n",
    "- **Noise reduction**: By discarding components with tiny variance, PCA filters out noise that is uncorrelated across neurons, yielding cleaner latent activity traces.\n",
    "- **Population motifs**: Components often correspond to co-activated neuronal ensembles or behavioral epochs, helping uncover coordinated circuit motifs.\n",
    "- **Downstream convenience**: PCA embeddings serve as inputs for visualization, clustering, or regression models that prefer compact feature spaces.\n",
    "\n",
    "### Interpreting PCA output\n",
    "- **Explained variance ratio** quantifies the fraction of total variance captured by each component. A sharp drop indicates an intrinsic low-dimensional structure.\n",
    "- **Loadings (eigenvectors)** tell you how strongly each neuron contributes to a component. Plotting loadings alongside anatomical locations can reveal spatial structure.\n",
    "- **Scores (projected data)** are time courses in the latent space that you can relate to behavior or stimuli.\n",
    "\n",
    "Remember that PCA assumes linear relationships and is sensitive to scaling. Standardizing or otherwise normalizing the traces before PCA is essential if neurons have widely different amplitudes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9db592",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_choice == \"dendrites\":\n",
    "    print(\"\\nDataset: Dendritic tree activity during behavior\")\n",
    "    # remove global median of all cells from each neuron\n",
    "    calcium_norm = calcium_data - np.median(calcium_data, axis=0)\n",
    "else:\n",
    "    print(\"\\nDataset: Population activity during behavior\")\n",
    "    # classic neuroanl standarization: ΔF/F0\n",
    "    F0 = np.percentile(calcium_data, 20, axis=1, keepdims=True)\n",
    "    calcium_norm = (calcium_data - F0) / F0 # ΔF/F0\n",
    "\n",
    "# show media subtracted data\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(\n",
    "    calcium_norm,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"viridis\",\n",
    "    extent=[time_axis[0], time_axis[-1], 0, n_neurons],\n",
    "    vmin=-2,\n",
    "    vmax=2,\n",
    "    interpolation='none',  # Prevent line blending/smoothing\n",
    "    )\n",
    "cbar = plt.colorbar(orientation=\"horizontal\", pad=0.2, label=\"ΔF/F0 (median subtracted)\", shrink=0.3)\n",
    "cbar.outline.set_visible(False)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Neuron #\")\n",
    "plt.title(\"Neural activity (median subtracted)\")\n",
    "sns.despine()\n",
    "plt.grid(False)\n",
    "\n",
    "print(\"\\n3. Principal Component Analysis...\")\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca_result = pca.fit_transform(calcium_norm)\n",
    "\n",
    "# Figure 3: 3D PCA scatter plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "scatter = ax1.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], \n",
    "                     c=range(n_neurons), cmap='Set2', s=40)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.set_title('First 3 components capture main data variance')\n",
    "sns.despine()\n",
    "ax1.grid(False)\n",
    "\n",
    "# Figure 4: Explained variance plot and cumulative explained variance\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(range(1, 11), pca.explained_variance_ratio_*100, 'bo-', linewidth=1, markersize=6)\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Explained Variance (%)')\n",
    "ax2.set_title('How many components do we need?')\n",
    "\n",
    "# Cumulative explained variance on right axis\n",
    "ax2_right = ax2.twinx()\n",
    "ax2_right.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_)*100, 'ro-', linewidth=1, markersize=6)\n",
    "ax2_right.set_ylabel('Cumulative Explained Variance (%)', color='red')\n",
    "ax2_right.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax2.legend(['Explained Variance'], loc='upper left')\n",
    "ax2_right.legend(['Cumulative Variance'], loc='upper right')\n",
    "ax2.grid(False)\n",
    "sns.despine(top=True,right=False)\n",
    "\n",
    "# Cap to 100%\n",
    "ax2_right.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"First 3 PCs explain {pca.explained_variance_ratio_[:3].sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4316df",
   "metadata": {},
   "source": [
    "# 3. NON-LINEAR DIMENSIONALITY REDUCTION\n",
    "\n",
    "### When linear methods fall short\n",
    "PCA excels when the data lie near a linear subspace, but neural activity often evolves on curved manifolds: think of trajectories that wrap around during cyclic behaviors or states that branch as animals switch tasks. Non-linear techniques attempt to preserve more nuanced geometry—local neighborhoods, geodesic distances, or global topology—that linear projections flatten.\n",
    "\n",
    "### Two commonly used methods\n",
    "- **t-SNE (t-distributed Stochastic Neighbor Embedding)** emphasizes local neighborhood preservation. It is excellent for visualizing clusters but can distort global distances. Perplexity controls the effective number of neighbors; values between 5 and 50 usually work, but tuning is encouraged.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)** balances local and global structure, often running faster than t-SNE while producing stable embeddings. The `n_neighbors` parameter behaves similarly to perplexity, while `min_dist` adjusts how tightly points are packed.\n",
    "\n",
    "### Practical guidance\n",
    "- Always feed these methods a reasonably denoised representation (e.g., the top PCs) to avoid wasting effort on noise.\n",
    "- Run multiple random seeds to check that qualitative structures persist; both methods involve randomness.\n",
    "- Interpret relative positions, not absolute axes—rotations or reflections of the embedding are arbitrary.\n",
    "\n",
    "Non-linear embeddings are powerful storytelling tools but should be complemented with quantitative analyses to avoid over-interpreting visualization artifacts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved t-SNE: run on first 10 PCA components, increase perplexity, and add explanation\n",
    "print(\"\\n4. Non-linear Dimensionality Reduction...\")\n",
    "\n",
    "# PCA preprocessing for t-SNE (denoising, speed)\n",
    "pca_for_tsne = PCA(n_components=10)\n",
    "pca_data = pca_for_tsne.fit_transform(calcium_norm)\n",
    "\n",
    "# Try higher perplexity (20-40 typical for moderate datasets)\n",
    "tsne = TSNE(n_components=3, perplexity=3, learning_rate=10, init='pca')\n",
    "tsne_result = tsne.fit_transform(pca_data)\n",
    "\n",
    "# Figure: Compare PCA vs improved t-SNE\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=range(n_neurons), cmap='Set2', s=40)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA (Linear)')\n",
    "cbar = plt.colorbar()\n",
    "cbar.outline.set_visible(False)\n",
    "sns.despine()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=range(n_neurons), cmap='Set2', s=40)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('Improved t-SNE (Non-linear structure)')\n",
    "cbar = plt.colorbar()\n",
    "cbar.outline.set_visible(False)\n",
    "sns.despine()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comment: t-SNE may still show poor structure if the data is noisy or lacks clear clusters. Try different perplexity values or cluster numbers for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23805261",
   "metadata": {},
   "source": [
    "Next, we build a “consensus” UMAP embedding by running UMAP multiple times with different random seeds, aligning the results, and aggregating them to emphasize structures that are stable across runs. The helper function consensus_umap_procrustes takes a feature matrix X (here, the PCA-preprocessed neural data), runs UMAP runs times and obtains an embedding Y each time. To make the embeddings comparable, each Y is centered and globally L2-normalized (subtract mean, divide by the Frobenius norm), removing translation and scale. The first embedding becomes the reference. Subsequent embeddings are orthogonally aligned to the reference via Procrustes: compute the SVD of $Y^T\\,ref = U\\Sigma V^T$ and rotate with $R = U V^T$, i.e., use $Y R$ to remove arbitrary rotations/reflections introduced by UMAP. After aligning all runs, the function stacks the embeddings and returns the pointwise median across runs, which is robust to outlier runs and highlights consistent geometry. The default n_neighbors=3 and min_dist=0.1 bias UMAP toward very local structure and tight clusters, which can be useful but may fragment data if the neighborhood is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConsensus UMAP: Multiple runs to highlight stable structure\")\n",
    "# Consensus UMAP: run UMAP multiple times and visualize stable structure\n",
    "\n",
    "def consensus_umap_procrustes(X, runs=20, n_neighbors=3, min_dist=0.1, seed=0, n_components=2):\n",
    "    embs, ref = [], None\n",
    "    for i in range(runs):\n",
    "        print(f\"Run {i+1}/{runs}\")\n",
    "        Y = UMAP(n_components=n_components, n_neighbors=n_neighbors,\n",
    "                 min_dist=min_dist, random_state=seed+i).fit_transform(X)\n",
    "        Y -= Y.mean(0); Y /= np.sqrt((Y**2).sum())      # center/normalize\n",
    "        if ref is None: ref = Y\n",
    "        U, _, Vt = np.linalg.svd(Y.T @ ref, full_matrices=False)\n",
    "        embs.append(Y @ (U @ Vt))                       # orthogonal alignment\n",
    "    return np.median(np.stack(embs), axis=0)\n",
    "\n",
    "consensus_emb = consensus_umap_procrustes(pca_data)  # -> (n_samples, 2)\n",
    "\n",
    "\n",
    "# Figure: Compare PCA vs improved UMAP\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PCA plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=range(n_neurons), cmap='Set2', s=40)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('PCA')\n",
    "sns.despine()\n",
    "plt.grid(False)\n",
    "\n",
    "# t-SNE plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(consensus_emb[:, 0], consensus_emb[:, 1], c=range(n_neurons), cmap='Set2', s=40)\n",
    "plt.xlabel('Consensus UMAP 1')\n",
    "plt.ylabel('Consensus UMAP 2')\n",
    "plt.title('Consensus UMAP (average of multiple runs)')\n",
    "cbar = plt.colorbar(label=\"Neuron #\")\n",
    "cbar.outline.set_visible(False)\n",
    "sns.despine()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60460b8",
   "metadata": {},
   "source": [
    "# 4. CORRELATION ANALYSIS & CLUSTERING\n",
    "\n",
    "### Why cluster neurons?\n",
    "Clustering transforms a sea of individual traces into a handful of representative groups, making it easier to reason about circuit motifs and to link them with behavior. For population imaging, clusters can correspond to neurons that fire together, neurons driven by the same stimulus, or anatomically co-located cells.\n",
    "\n",
    "### Two complementary perspectives\n",
    "1. **Correlation-based clustering**: Operates directly on the activity matrix by comparing full time courses (e.g., Pearson correlation). Hierarchical agglomerative clustering is a natural fit: it builds a dendrogram that records how neurons merge as you relax the similarity threshold, and you can “cut” the tree at different heights to explore candidate cluster counts.\n",
    "2. **Embedding-based clustering**: Works on low-dimensional coordinates (e.g., PCA, t-SNE, UMAP). Density-based methods such as HDBSCAN identify tight groups while flagging sparse points as noise, which is valuable when populations contain both structured ensembles and idiosyncratic neurons.\n",
    "\n",
    "### Good practices\n",
    "- Standardize or z-score traces before computing correlations so that differences in variance do not dominate similarity.\n",
    "- Compare multiple linkage criteria (ward, average, complete) in hierarchical clustering; each encodes a different notion of cluster compactness.\n",
    "- For HDBSCAN, examine both the labels and the membership strength probabilities to decide which neurons belong confidently to each cluster.\n",
    "\n",
    "Clustering is exploratory by nature. Use it to generate hypotheses, then verify them through anatomical overlays, stimulus alignment, or follow-up experiments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4807344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improved Correlation Analysis & Clustering\n",
    "print(\"\\n5. Improved Correlation Analysis & Clustering...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# --- Config ---\n",
    "corr_kind = 'pearson'       # 'pearson' or 'spearman'\n",
    "linkage_method = 'average'  # default for non-Euclidean option\n",
    "use_ward = True             # False → produce Z_abs & Z_signed; True → use Ward (Euclidean) and produce Z\n",
    "\n",
    "# --- Correlation (neurons x neurons) ---\n",
    "if corr_kind == 'pearson':\n",
    "    corr_matrix = np.corrcoef(calcium_norm)\n",
    "else:\n",
    "    corr_matrix = spearmanr(calcium_norm.T).correlation\n",
    "\n",
    "# Clean + enforce symmetry & unit diagonal\n",
    "corr_matrix = np.nan_to_num(np.clip(corr_matrix, -1.0, 1.0), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "corr_matrix = 0.5 * (corr_matrix + corr_matrix.T)\n",
    "np.fill_diagonal(corr_matrix, 1.0)\n",
    "\n",
    "# --- Distances from correlation ---\n",
    "# Sign-invariant (anti-correlated = near)\n",
    "dist_abs = 1.0 - np.abs(corr_matrix)\n",
    "np.fill_diagonal(dist_abs, 0.0)\n",
    "dist_abs = 0.5 * (dist_abs + dist_abs.T)\n",
    "\n",
    "# Sign-sensitive (anti-correlated = far)\n",
    "dist_signed = 1.0 - corr_matrix\n",
    "np.fill_diagonal(dist_signed, 0.0)\n",
    "dist_signed = 0.5 * (dist_signed + dist_signed.T)\n",
    "\n",
    "# Euclidean-compatible (for 'ward'/'centroid'/'median')\n",
    "dist_chord = np.sqrt(2.0 * (1.0 - corr_matrix))\n",
    "np.fill_diagonal(dist_chord, 0.0)\n",
    "dist_chord = 0.5 * (dist_chord + dist_chord.T)\n",
    "\n",
    "# --- Linkage (if/else option) ---\n",
    "if use_ward:\n",
    "    # If you switch to Ward/centroid/median, use dist_chord:\n",
    "    linkage_method = 'ward'\n",
    "    Z = linkage(squareform(dist_chord, checks=True), method='ward')\n",
    "else:\n",
    "    Z_abs    = linkage(squareform(dist_abs,    checks=True), method=linkage_method)\n",
    "    Z_signed = linkage(squareform(dist_signed, checks=True), method=linkage_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def find_optimal_clusters_simple(Z, corr_matrix, max_k=15):\n",
    "    \"\"\"Find optimal clusters using silhouette analysis - most robust single method.\"\"\"\n",
    "    # Convert correlation to distance matrix (diagonal = 0)\n",
    "    dist_matrix = 1 - np.abs(corr_matrix)\n",
    "    np.fill_diagonal(dist_matrix, 0)\n",
    "    \n",
    "    # Calculate silhouette scores\n",
    "    k_range = range(2, min(max_k+1, corr_matrix.shape[0]))\n",
    "    scores = [silhouette_score(dist_matrix, fcluster(Z, k, criterion='maxclust'), metric='precomputed') \n",
    "              for k in k_range]\n",
    "    \n",
    "    optimal_k = k_range[np.argmax(scores)]\n",
    "    \n",
    "    # Create two plots: score vs k, and detailed silhouette plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Silhouette score vs number of clusters\n",
    "    ax1.plot(k_range, scores, 'bo-', linewidth=2, markersize=8)\n",
    "    ax1.axvline(optimal_k, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    ax1.set_title('Silhouette Analysis', fontsize=14)\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Average Silhouette Score')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Detailed silhouette plot for optimal k\n",
    "    optimal_clusters = fcluster(Z, optimal_k, criterion='maxclust')\n",
    "    silhouette_vals = silhouette_samples(dist_matrix, optimal_clusters, metric='precomputed')\n",
    "    \n",
    "    y_lower = 10\n",
    "    colors = cm.nipy_spectral(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    for i, color in zip(range(1, optimal_k + 1), colors):\n",
    "        cluster_silhouette_vals = silhouette_vals[optimal_clusters == i]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        \n",
    "        size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        # Label clusters at center\n",
    "        ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax2.axvline(x=avg_score, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "    ax2.set_xlabel('Silhouette Coefficient Values')\n",
    "    ax2.set_ylabel('Cluster Label')\n",
    "    ax2.set_title(f'Silhouette Plot for {optimal_k} Clusters\\n(Avg Score: {avg_score:.3f})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find corresponding dendrogram threshold\n",
    "    merge_idx = len(Z) - optimal_k + 1\n",
    "    threshold = Z[merge_idx, 2] if merge_idx < len(Z) else Z[-1, 2]\n",
    "    \n",
    "    return optimal_k, threshold, max(scores)\n",
    "\n",
    "# Usage:\n",
    "optimal_k, threshold, best_score = find_optimal_clusters_simple(Z, corr_matrix)\n",
    "print(f\"Optimal: {optimal_k} clusters | Threshold: {threshold:.3f} | Silhouette: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f826e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dendrogram(s) ---\n",
    "if use_ward:\n",
    "    # Single dendrogram for Ward\n",
    "    color_threshold = threshold #* np.max(Z[:, 2])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    dendrogram(Z, no_labels=True, leaf_rotation=0, color_threshold=color_threshold)\n",
    "    plt.title('Ward linkage (Euclidean chord distance)', fontsize=12, pad=10)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Two dendrograms: abs vs signed\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    color_threshold_abs = threshold #* np.max(Z_abs[:, 2])\n",
    "    color_threshold_signed = threshold #* np.max(Z_signed[:, 2])\n",
    "\n",
    "    dendrogram(Z_abs, no_labels=True, ax=axes[0], leaf_rotation=0, color_threshold=color_threshold_abs)\n",
    "    axes[0].set_title('Absolute Distance (1 - |r|)', fontsize=12, pad=10)\n",
    "    axes[0].spines['top'].set_visible(False); axes[0].spines['right'].set_visible(False)\n",
    "    axes[0].tick_params(labelsize=10)\n",
    "\n",
    "    dendrogram(Z_signed, no_labels=True, ax=axes[1], leaf_rotation=0, color_threshold=color_threshold_signed)\n",
    "    axes[1].set_title('Signed Distance (1 - r)', fontsize=12, pad=10)\n",
    "    axes[1].spines['top'].set_visible(False); axes[1].spines['right'].set_visible(False)\n",
    "    axes[1].tick_params(labelsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Sorted matrices, cluster extraction, and overlays\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy.cluster.hierarchy import fcluster, leaves_list\n",
    "\n",
    "def contig_blocks(labels):\n",
    "    \"\"\"Yield (start, end, value) for contiguous runs in a 1D label array.\"\"\"\n",
    "    n = len(labels)\n",
    "    if n == 0: \n",
    "        return\n",
    "    start = 0\n",
    "    cur = labels[0]\n",
    "    for i in range(1, n):\n",
    "        if labels[i] != cur:\n",
    "            yield start, i, cur\n",
    "            start = i\n",
    "            cur = labels[i]\n",
    "    yield start, n, cur\n",
    "\n",
    "def add_cluster_overlays(ax, clusters_sorted, alpha=0.15):\n",
    "    \"\"\"Add translucent squares for contiguous cluster blocks on a heatmap.\"\"\"\n",
    "    unique_clusters = np.unique(clusters_sorted)\n",
    "    palette = ['red','blue','green','orange','purple','brown','pink','gray','olive','cyan']\n",
    "    color_map = {c: palette[i % len(palette)] for i, c in enumerate(unique_clusters)}\n",
    "    for s, e, c in contig_blocks(clusters_sorted):\n",
    "        size = e - s\n",
    "        # outline\n",
    "        rect = patches.Rectangle((s-0.5, s-0.5), size, size, \n",
    "                                 linewidth=2, edgecolor=color_map[c], facecolor='none', alpha=1.0)\n",
    "        ax.add_patch(rect)\n",
    "        # fill\n",
    "        rect_fill = patches.Rectangle((s-0.5, s-0.5), size, size, \n",
    "                                      linewidth=0, facecolor=color_map[c], alpha=alpha)\n",
    "        ax.add_patch(rect_fill)\n",
    "\n",
    "if use_ward:\n",
    "    # ----- Ward mode: one Z (Euclidean), one sorting, one cluster vector -----\n",
    "    # Leaves / order\n",
    "    sort_idx = leaves_list(Z)\n",
    "    corr_sorted = corr_matrix[sort_idx][:, sort_idx]\n",
    "\n",
    "    # Threshold & clusters (match first-cell convention: 70% of max merge height)\n",
    "    color_threshold = threshold #* np.max(Z[:, 2])\n",
    "    clusters = fcluster(Z, color_threshold, criterion='distance')\n",
    "    clusters_sorted = clusters[sort_idx]\n",
    "\n",
    "    # Plot original vs Ward-sorted with overlays\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4.5))\n",
    "    sns.heatmap(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, ax=axes[0], xticklabels=False, yticklabels=False)\n",
    "    axes[0].set_title(\"Original Matrix\", fontsize=12, pad=10)\n",
    "\n",
    "    sns.heatmap(corr_sorted, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, ax=axes[1], xticklabels=False, yticklabels=False)\n",
    "    add_cluster_overlays(axes[1], clusters_sorted, alpha=0.15)\n",
    "    axes[1].set_title(\"Ward-sorted (chord distance) + Clusters\", fontsize=12, pad=10)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Stats & quick visuals\n",
    "    print(f\"\\nUsing {corr_kind} correlation. Ward linkage.\")\n",
    "    print(f\"Color threshold (Ward): {color_threshold:.3f}\")\n",
    "    uniq, counts = np.unique(clusters_sorted, return_counts=True)\n",
    "    print(f\"Ward clustering: {len(uniq)} clusters\")\n",
    "    print(f\"Cluster sizes: {counts.tolist()}\")\n",
    "else:\n",
    "    # ----- Non-Euclidean mode: abs & signed -----\n",
    "    # Leaves / orders\n",
    "    sort_idx_abs = leaves_list(Z_abs)\n",
    "    sort_idx_signed = leaves_list(Z_signed)\n",
    "\n",
    "    # Sorted matrices\n",
    "    corr_sorted_abs = corr_matrix[sort_idx_abs][:, sort_idx_abs]\n",
    "    corr_sorted_signed = corr_matrix[sort_idx_signed][:, sort_idx_signed]\n",
    "\n",
    "    # Thresholds & clusters (70% of max height)\n",
    "    color_threshold_abs = threshold #* np.max(Z_abs[:, 2])\n",
    "    color_threshold_signed = threshold #* np.max(Z_signed[:, 2])\n",
    "    clusters_abs = fcluster(Z_abs, color_threshold_abs, criterion='distance')[sort_idx_abs]\n",
    "    clusters_signed = fcluster(Z_signed, color_threshold_signed, criterion='distance')[sort_idx_signed]\n",
    "\n",
    "    # Heatmaps with overlays\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "    sns.heatmap(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, ax=axes[0], xticklabels=False, yticklabels=False)\n",
    "    axes[0].set_title(\"Original Matrix\", fontsize=12, pad=10)\n",
    "\n",
    "    sns.heatmap(corr_sorted_abs, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, ax=axes[1], xticklabels=False, yticklabels=False)\n",
    "    add_cluster_overlays(axes[1], clusters_abs, alpha=0.15)\n",
    "    axes[1].set_title(\"Sorted by |Correlation| + Clusters\", fontsize=12, pad=10)\n",
    "\n",
    "    sns.heatmap(corr_sorted_signed, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, ax=axes[2], xticklabels=False, yticklabels=False)\n",
    "    add_cluster_overlays(axes[2], clusters_signed, alpha=0.15)\n",
    "    axes[2].set_title(\"Sorted by Correlation + Clusters\", fontsize=12, pad=10)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Stats\n",
    "    print(f\"\\nUsing {corr_kind} correlation.\")\n",
    "    print(f\"Color threshold (abs): {color_threshold_abs:.3f}\")\n",
    "    uniq_abs, cnt_abs = np.unique(clusters_abs, return_counts=True)\n",
    "    print(f\"Absolute distance clustering: {len(uniq_abs)} clusters\")\n",
    "    print(f\"Cluster sizes (abs): {cnt_abs.tolist()}\")\n",
    "\n",
    "    print(f\"\\nColor threshold (signed): {color_threshold_signed:.3f}\")\n",
    "    uniq_sig, cnt_sig = np.unique(clusters_signed, return_counts=True)\n",
    "    print(f\"Signed distance clustering: {len(uniq_sig)} clusters\")\n",
    "    print(f\"Cluster sizes (signed): {cnt_sig.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN on UMAP coordinates + correlation reordered by UMAP clusters\n",
    "print(\"\\n5. Clustering (HDBSCAN on UMAP) and corr matrix reordered by clusters...\")\n",
    "\n",
    "# Run HDBSCAN on the consensus UMAP embedding\n",
    "clusterer = HDBSCAN(min_cluster_size=5)\n",
    "cluster_labels = clusterer.fit_predict(consensus_emb)\n",
    "\n",
    "# Visualization: UMAP colored by HDBSCAN clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    consensus_emb[:, 0], consensus_emb[:, 1],\n",
    "    c=cluster_labels, cmap='tab10', alpha=0.8, s=30\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Cluster\", shrink=0.8)\n",
    "plt.title('UMAP with HDBSCAN Clusters', fontsize=14, pad=15)\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Build correlation matrix and reorder by clusters\n",
    "corr_matrix = np.corrcoef(calcium_norm)\n",
    "\n",
    "# Reorder indices by cluster label (noise = -1 at end)\n",
    "unique_labels = [c for c in np.unique(cluster_labels) if c != -1]\n",
    "ordered_indices = []\n",
    "for c in unique_labels:\n",
    "    ordered_indices.append(np.where(cluster_labels == c)[0])\n",
    "noise_indices = np.where(cluster_labels == -1)[0]\n",
    "if noise_indices.size > 0:\n",
    "    ordered_indices.append(noise_indices)\n",
    "sort_idx = np.concatenate(ordered_indices) if len(ordered_indices) else np.arange(corr_matrix.shape[0])\n",
    "\n",
    "corr_sorted = corr_matrix[sort_idx][:, sort_idx]\n",
    "\n",
    "# Comparison: original vs cluster-sorted correlation matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8}, ax=axes[0], \n",
    "            xticklabels=False, yticklabels=False)\n",
    "axes[0].set_title(\"Original Matrix\", fontsize=12, pad=10)\n",
    "\n",
    "sns.heatmap(corr_sorted, cmap=\"RdBu_r\", vmin=-1, vmax=1, square=True,\n",
    "            cbar_kws={\"shrink\": 0.8}, ax=axes[1],\n",
    "            xticklabels=False, yticklabels=False)  \n",
    "axes[1].set_title(\"Sorted by HDBSCAN Clusters\", fontsize=12, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Found {len(unique_labels)} clusters, {sum(cluster_labels == -1)} noise points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4fbef",
   "metadata": {},
   "source": [
    "# 5. CLUSTER VALIDATION & INTERPRETATION\n",
    "\n",
    "### Guarding against over-interpretation\n",
    "Clustering algorithms will always partition the data—even pure noise can yield seemingly neat groups. Validation is therefore critical before we claim the existence of functional ensembles. Ask whether clusters are reproducible across methods, stable under resampling, and interpretable within the biological context.\n",
    "\n",
    "### Validation toolbox\n",
    "1. **Visual diagnostics**: Inspect embeddings colored by cluster labels. Coherent, well-separated clouds inspire more confidence than fuzzy gradients.\n",
    "2. **Cross-method agreement**: Compare labels from correlation-based and embedding-based approaches (e.g., via adjusted Rand index). Convergence across methods suggests signal rather than artifacts.\n",
    "3. **Temporal and behavioral profiles**: Compute average traces or stimulus-triggered responses per cluster. Distinct dynamics imply functional specialization.\n",
    "4. **Null models**: Randomly shuffle time or neuron identities and rerun clustering to ensure the observed structure exceeds what noise would produce.\n",
    "\n",
    "### From clusters to neuroscience insight\n",
    "Once confident in cluster validity, map clusters back onto anatomy, cell-type markers, or behavioral epochs. Look for enrichment of known cell classes or alignment with behaviorally relevant times. Treat clusters as hypotheses: design follow-up experiments to test whether the grouped neurons indeed share synaptic inputs, genetic identity, or behavioral roles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. Cluster Validation...\")\n",
    "\n",
    "# Prepare both clustering labelings\n",
    "labels_umap = cluster_labels  # HDBSCAN on UMAP\n",
    "labels_hier = None\n",
    "\n",
    "# Recover hierarchical labels by cutting dendrogram\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    if 'linkage_matrix' not in globals():\n",
    "        corr_matrix = np.corrcoef(calcium_norm)\n",
    "        linkage_matrix = linkage(corr_matrix, method='average')\n",
    "    \n",
    "    k_umap = len([c for c in np.unique(labels_umap) if c != -1])\n",
    "    k_hier = max(k_umap, 2) if k_umap > 0 else 2\n",
    "    labels_hier = fcluster(linkage_matrix, k_hier, criterion='maxclust') - 1\n",
    "except Exception as e:\n",
    "    print(f\"Warning: hierarchical labels could not be computed: {e}\")\n",
    "    labels_hier = np.zeros_like(labels_umap)\n",
    "\n",
    "# Compute cluster-average traces\n",
    "valid_idx_umap = labels_umap != -1\n",
    "unique_umap = [c for c in np.unique(labels_umap) if c != -1]\n",
    "unique_hier = np.unique(labels_hier)\n",
    "\n",
    "means_umap = np.array([calcium_norm[labels_umap == c].mean(axis=0) for c in unique_umap])\n",
    "means_hier = np.array([calcium_norm[labels_hier == c].mean(axis=0) for c in unique_hier])\n",
    "\n",
    "# Plot cluster traces\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# UMAP-HDBSCAN traces\n",
    "for j, c in enumerate(unique_umap):\n",
    "    axes[0].plot(time_axis, means_umap[j] + j*2.0, linewidth=0.5, alpha=0.8,\n",
    "                label=f'C{c} (n={sum(labels_umap==c)})')\n",
    "axes[0].set_ylabel('Activity (offset)', fontsize=12)\n",
    "axes[0].set_title('UMAP-HDBSCAN Cluster Traces', fontsize=13, pad=15)\n",
    "axes[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)\n",
    "axes[0].tick_params(labelsize=10)\n",
    "\n",
    "# Hierarchical traces  \n",
    "for j, c in enumerate(unique_hier):\n",
    "    axes[1].plot(time_axis, means_hier[j] + j*5, linewidth=0.5, alpha=0.8,\n",
    "                label=f'C{c} (n={sum(labels_hier==c)})')\n",
    "axes[1].set_ylabel('Activity (offset)', fontsize=12)\n",
    "axes[1].set_xlabel('Time', fontsize=12)\n",
    "axes[1].set_title('Hierarchical Cluster Traces', fontsize=13, pad=15)\n",
    "axes[1].legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)\n",
    "axes[1].tick_params(labelsize=10)\n",
    "\n",
    "plt.setp([ax.spines.values() for ax in axes], linewidth=0.5)\n",
    "for ax in axes:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix between methods\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mask = labels_umap != -1\n",
    "if mask.sum() > 0:\n",
    "    relabel_map = {c: i for i, c in enumerate(unique_umap)}\n",
    "    labels_umap_relab = np.array([relabel_map[c] for c in labels_umap[mask]])\n",
    "    labels_hier_sel = labels_hier[mask]\n",
    "    conf_mat = confusion_matrix(labels_umap_relab, labels_hier_sel)\n",
    "else:\n",
    "    conf_mat = np.zeros((1, 1), dtype=int)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "            cbar_kws={'shrink': 0.8}, annot_kws={'fontsize': 11})\n",
    "plt.title('UMAP-HDBSCAN vs Hierarchical Clustering', fontsize=13, pad=15)\n",
    "plt.xlabel('Hierarchical Cluster', fontsize=12)\n",
    "plt.ylabel('UMAP-HDBSCAN Cluster', fontsize=12)\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Dataset: {n_neurons} neurons, {n_timepoints} timepoints\")\n",
    "print(f\"PCA: First 3 components = {pca.explained_variance_ratio_[:3].sum()*100:.1f}% variance\")\n",
    "print(f\"UMAP-HDBSCAN: {len(unique_umap)} clusters + {sum(labels_umap==-1)} noise points\")\n",
    "print(f\"Hierarchical: {len(unique_hier)} clusters\")\n",
    "\n",
    "print(f\"\\n=== CLUSTER SIZES ===\")\n",
    "for c in unique_umap:\n",
    "    print(f\"  UMAP C{c}: {sum(labels_umap==c)} neurons\")\n",
    "for c in unique_hier:\n",
    "    print(f\"  Hier C{c}: {sum(labels_hier==c)} neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8c586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SummerSchool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

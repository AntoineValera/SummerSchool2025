{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1fcb8e",
   "metadata": {},
   "source": [
    "# Hands-On General Statistics for Neuronal Data\n",
    "\n",
    "Welcome to a guided tour of first-pass statistics for two-photon calcium imaging. The notebook mirrors the narrative style of our other hands-on chapters: each section introduces the scientific question, explains the statistical tooling, and leaves behind code you can reuse. We will:\n",
    "\n",
    "* load preprocessed fluorescence and behavioural traces from MATLAB files;\n",
    "* establish a drift-resistant baseline to compute $\\Delta F/F$ and robust $z$-scores;\n",
    "* extract discrete events with a transparent peak detector; and\n",
    "* summarise event statistics and their coupling to behaviour.\n",
    "\n",
    "Throughout we emphasise reproducible defaults, parameterisations grounded in signal-processing heuristics, and visual checks that highlight when further modelling might be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa928583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# --- Scientific Python ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', context='talk', palette='colorblind')\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.linewidth': 0.6,\n",
    "})\n",
    "print('=== GENERAL STATISTICS FOR NEURAL DATA ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978eb69a",
   "metadata": {},
   "source": [
    "## Load fluorescence and behaviour traces\n",
    "\n",
    "The `Data/` directory ships with two example recordings exported as MATLAB `.mat` files:\n",
    "\n",
    "* **`population`** – a mesoscopic field of view containing dozens of somata and a simultaneously acquired behavioural regressor (think running speed or lever pressure).\n",
    "* **`dendrites`** – a tighter field around a handful of dendritic segments with correspondingly lower signal-to-noise ratio.\n",
    "\n",
    "Select the dataset via `dataset_choice` to toggle which file is loaded. Each file exposes a dictionary with:\n",
    "\n",
    "* `traces`: fluorescence matrix of shape `(n_neurons, n_timepoints)`;\n",
    "* `time`: sampling grid in seconds;\n",
    "* `behaviour`: continuous regressor aligned to `time`;\n",
    "* optional metadata such as sampling rate and stimulus annotations.\n",
    "\n",
    "Loading is handled by `scipy.io.loadmat`, after which we standardise key field names to ease downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca826261",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('Data')\n",
    "dataset_choice = 'population'  # 'population' or 'dendrites'\n",
    "\n",
    "if dataset_choice == 'population':\n",
    "    mat = loadmat(DATA_DIR / 'M1_population_data.mat')\n",
    "    mat_data = mat['mat_data']\n",
    "    F = np.asarray(mat_data['result'][0, 0], dtype=float)\n",
    "    behaviour = np.asarray(mat_data['behaviour'][0, 0], dtype=float).squeeze()\n",
    "    time = np.asarray(mat_data['tax'][0, 0], dtype=float).squeeze()\n",
    "    if 'fs' in mat_data.dtype.names:\n",
    "        fs = float(np.atleast_1d(mat_data['fs'][0, 0]).squeeze())\n",
    "    else:\n",
    "        dt = float(np.median(np.diff(time))) if time.size > 1 else np.nan\n",
    "        fs = float(1.0 / dt) if np.isfinite(dt) and dt > 0 else 10.0\n",
    "elif dataset_choice == 'dendrites':\n",
    "    mat = loadmat(DATA_DIR / 'M1_dendritic_tree_data.mat')\n",
    "    F = np.asarray(mat['result'], dtype=float)\n",
    "    behaviour = np.asarray(mat['behaviour'], dtype=float).squeeze()\n",
    "    time = np.asarray(mat['tax'], dtype=float).squeeze()\n",
    "    dt = float(np.median(np.diff(time))) if time.size > 1 else np.nan\n",
    "    fs = float(1.0 / dt) if np.isfinite(dt) and dt > 0 else 10.0\n",
    "else:\n",
    "    raise ValueError('Unknown dataset choice')\n",
    "\n",
    "# Ensure shape is (n_cells, n_time)\n",
    "F = np.asarray(F, dtype=float)\n",
    "if F.shape[0] > F.shape[1]:\n",
    "    F = F.T\n",
    "n_cells, n_time = F.shape\n",
    "behaviour = np.asarray(behaviour, dtype=float).reshape(-1)[:n_time]\n",
    "time = np.asarray(time, dtype=float).reshape(-1)[:n_time]\n",
    "if time.size != n_time:\n",
    "    time = np.arange(n_time) / fs\n",
    "\n",
    "time_axis = time\n",
    "recording_duration = n_time / fs if fs else np.nan\n",
    "\n",
    "print(f'Dataset: {dataset_choice}')\n",
    "print(f'Cells: {n_cells}, Time points: {n_time}, Sampling rate: {fs:.2f} Hz')\n",
    "print(f'Recording duration: {recording_duration / 60:.1f} minutes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b50f0",
   "metadata": {},
   "source": [
    "![See Causal_smooth.py](image.png)\n",
    "\n",
    "*Figure –* Schematic of the causal smoothing kernel leveraged later for baseline estimation. Keep this intuition in mind when inspecting slow drifts versus transient events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_trace_causal(trace, fs, sigma_sec=1.0):\n",
    "    sigma_samples = max(1, int(round(sigma_sec * fs)))\n",
    "    t = np.arange(4 * sigma_samples + 1)\n",
    "    kernel = np.exp(-0.5 * (t / sigma_samples) ** 2)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    padded = np.concatenate([np.zeros(len(kernel)-1), trace])\n",
    "    return np.convolve(padded, kernel, mode='valid')\n",
    "\n",
    "F = np.asarray([smooth_trace_causal(F[i], fs, sigma_sec=1.0) for i in range(n_cells)], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc0d53",
   "metadata": {},
   "source": [
    "### Why causal smoothing?\n",
    "\n",
    "The raw fluorescence traces contain shot noise and fast artefacts that hamper baseline estimation.\n",
    "The helper `smooth_trace_causal` applies a one-sided Gaussian kernel so each time point only depends on past\n",
    "samples—mimicking online preprocessing during acquisition. Feel free to adapt the kernel width if your sensor\n",
    "has faster kinetics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0b271",
   "metadata": {},
   "source": [
    "### Recording summary\n",
    "\n",
    "This panel materialises at-a-glance statistics for the active dataset:\n",
    "\n",
    "* number of regions of interest (ROIs) and total recording duration;\n",
    "* sampling frequency inferred from the `time` vector;\n",
    "* behavioural descriptors (mean level, standard deviation, and autocorrelation at one second).\n",
    "\n",
    "Treat this as a sanity check: unexpected sampling rates or missing behavioural channels usually indicate a mis-specified dataset key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_minutes = recording_duration / 60 if recording_duration else np.nan\n",
    "behaviour = np.asarray(behaviour, dtype=float)[:n_time]\n",
    "behaviour_mean = float(np.nanmean(behaviour))\n",
    "behaviour_std = float(np.nanstd(behaviour))\n",
    "lag_samples = int(round(fs)) if fs and fs > 0 else 0\n",
    "if lag_samples > 0 and len(behaviour) > lag_samples:\n",
    "    behaviour_autocorr = float(np.corrcoef(behaviour[:-lag_samples], behaviour[lag_samples:])[0, 1])\n",
    "else:\n",
    "    behaviour_autocorr = np.nan\n",
    "\n",
    "summary_table = pd.Series({\n",
    "    'cells': n_cells,\n",
    "    'timepoints': n_time,\n",
    "    'sampling_rate_hz': fs,\n",
    "    'duration_min': recording_minutes,\n",
    "    'behaviour_mean': behaviour_mean,\n",
    "    'behaviour_std': behaviour_std,\n",
    "    'behaviour_autocorr_1s': behaviour_autocorr,\n",
    "})\n",
    "display(summary_table.to_frame('value'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1935b5",
   "metadata": {},
   "source": [
    "### Quick look at raw fluorescence\n",
    "\n",
    "Before applying any preprocessing, visualise a subsample of raw traces. Calcium imaging is notorious for slow baseline drift, motion-induced artefacts, and variable noise floors across ROIs. The plots surface:\n",
    "\n",
    "* baseline wander motivating percentile-based detrending;\n",
    "* heteroscedastic noise: some neurons are much noisier than others;\n",
    "* shared fluctuations hinting at neuropil contamination or network-wide events.\n",
    "\n",
    "Use these diagnostics to decide whether neuropil subtraction or motion correction should precede this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = min(4, n_cells)\n",
    "example_indices = np.linspace(0, n_cells - 1, n_examples, dtype=int)\n",
    "fig, axes = plt.subplots(n_examples, 1, figsize=(12, 2.5 * n_examples), sharex=True)\n",
    "if n_examples == 1:\n",
    "    axes = [axes]\n",
    "for ax, idx in zip(axes, example_indices):\n",
    "    ax.plot(time_axis, F[idx], lw=1.0)\n",
    "    ax.set_ylabel(f'Cell {idx}')\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "fig.suptitle('Raw fluorescence traces', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5aa0d",
   "metadata": {},
   "source": [
    "## Robust baseline and normalisation\n",
    "\n",
    "We adopt a two-stage normalisation inspired by signal processing practice:\n",
    "\n",
    "1. **Baseline inference** – apply a causal, low-percentile filter (default 8th percentile in a multi-second window) to approximate the local resting fluorescence $F_0(t)$. This guards against contamination by large transients that would inflate a simple moving average.\n",
    "2. **Normalisation** – compute both $\\Delta F/F = (F - F_0) / F_0$ and a robust $z$-score, where the scale is estimated via the median absolute deviation (MAD) rescaled by $1.4826$ to target the Gaussian standard deviation.\n",
    "\n",
    "These choices mirror the helper utilities used across our hands-on series so that per-ROI operations remain independent. If your acquisition exhibits abrupt bleaching or dropped frames, consider adjusting the window length or swapping in an exponential moving minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_baseline_blocks(F, fs, block_sec=1.0, total_sec=5.0, step_sec=0.5):\n",
    "    \"\"\"\n",
    "    For each neuron, pick up to floor(total_sec/block_sec) NON-overlapping blocks\n",
    "    of length block_sec with the lowest within-block median.\n",
    "    Returns:\n",
    "      mu0:  (n,) - robust baseline mean (median)\n",
    "      sd0:  (n,) - standard deviation for proper Z-scores\n",
    "      mask: (n, T) boolean True inside selected blocks\n",
    "    \"\"\"\n",
    "    F = np.asarray(F, dtype=np.float32)\n",
    "    n, T = F.shape\n",
    "    block = max(1, int(round(block_sec * fs)))\n",
    "    max_blocks = max(1, int(np.floor(total_sec / block_sec)))\n",
    "    step = max(1, int(round(step_sec * fs)))\n",
    "\n",
    "    mu0 = np.empty(n, dtype=np.float32)\n",
    "    sd0 = np.empty(n, dtype=np.float32)\n",
    "    out_mask = np.zeros_like(F, dtype=bool)\n",
    "\n",
    "    starts = np.arange(0, max(0, T - block + 1), step)\n",
    "\n",
    "    for i in range(n):\n",
    "        x = F[i]\n",
    "        if starts.size == 0 or T < block:\n",
    "            mu = np.median(x)  # Robust baseline mean\n",
    "            mu0[i] = mu\n",
    "            sd0[i] = np.std(x)  # STANDARD DEVIATION for Z-score\n",
    "            continue\n",
    "\n",
    "        # Score each candidate window by its median\n",
    "        medians = np.array([np.median(x[s:s+block]) for s in starts], dtype=np.float32)\n",
    "        order = np.argsort(medians)\n",
    "\n",
    "        chosen = []\n",
    "        occupied = np.zeros(T, dtype=bool)\n",
    "        \n",
    "        for idx in order:\n",
    "            s = int(starts[idx])\n",
    "            e = min(s + block, T)\n",
    "            if not occupied[s:e].any():\n",
    "                chosen.append((s, e))\n",
    "                occupied[s:e] = True\n",
    "                if len(chosen) >= max_blocks:\n",
    "                    break\n",
    "\n",
    "        if not chosen:\n",
    "            mu = np.median(x)    # Robust baseline mean\n",
    "            sd = np.std(x)       # STANDARD DEVIATION for Z-score\n",
    "        else:\n",
    "            # Set mask for chosen blocks\n",
    "            for s, e in chosen:\n",
    "                out_mask[i, s:e] = True\n",
    "            vals = x[out_mask[i]]\n",
    "            mu = np.median(vals)  # Robust baseline mean\n",
    "            sd = np.std(vals)     # STANDARD DEVIATION for Z-score\n",
    "\n",
    "        mu0[i] = mu\n",
    "        sd0[i] = sd\n",
    "\n",
    "    return mu0, sd0, out_mask\n",
    "\n",
    "def mask_to_spans(mask_1d):\n",
    "    \"\"\"Boolean mask -> list of (start,end) index spans (half-open: end is exclusive).\"\"\"\n",
    "    if not mask_1d.any():\n",
    "        return []\n",
    "    \n",
    "    # Find transitions\n",
    "    padded = np.concatenate(([False], mask_1d, [False]))\n",
    "    diff = np.diff(padded.astype(int))\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    ends = np.where(diff == -1)[0]\n",
    "    \n",
    "    return list(zip(starts, ends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ca227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Compute baseline and Z-score ----\n",
    "mu0, sd0, base_mask = pick_baseline_blocks(F, fs, block_sec=30, total_sec=30.0, step_sec=0.5)\n",
    "\n",
    "# PROPER Z-SCORE COMPUTATION using standard deviation\n",
    "z = (F - mu0[:, None]) / (sd0[:, None] + 1e-6)\n",
    "\n",
    "# ---- Plot 5 cells with translucent red patches for the baseline blocks ----\n",
    "cells_to_plot = [0, 1, 2, 3, 4]\n",
    "fig, axes = plt.subplots(len(cells_to_plot), 1, figsize=(12, 8), sharex=True)\n",
    "if len(cells_to_plot) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, cell in zip(axes, cells_to_plot):\n",
    "    y = z[cell]\n",
    "    ax.plot(time_axis, y, lw=1.2, color='tab:green', zorder=2)\n",
    "    ax.axhline(0, color='0.3', lw=0.6, zorder=1)\n",
    "    ax.set_ylabel(f'Cell {cell}')\n",
    "\n",
    "    # Shaded baseline regions\n",
    "    spans = mask_to_spans(base_mask[cell])\n",
    "    for s, e in spans:\n",
    "        t_start = time_axis[s]\n",
    "        t_end = time_axis[e-1] if e > 0 else time_axis[s]\n",
    "        dt = time_axis[1] - time_axis[0] if len(time_axis) > 1 else 0\n",
    "        t_end += dt/2\n",
    "        ax.axvspan(t_start, t_end, color='red', alpha=0.3, lw=0, zorder=0)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "fig.suptitle(\"Z-scored traces (using std dev) with baseline blocks (red patches)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print diagnostics\n",
    "print(f\"Z-score computation verified:\")\n",
    "print(f\"  Baseline mean: median (robust)\")\n",
    "print(f\"  Baseline variability: standard deviation (proper Z-score)\")\n",
    "print(f\"\\nBaseline blocks found:\")\n",
    "for cell in cells_to_plot:\n",
    "    spans = mask_to_spans(base_mask[cell])\n",
    "    total_duration = sum(time_axis[e-1] - time_axis[s] for s, e in spans if e > s)\n",
    "    print(f\"  Cell {cell}: {len(spans)} blocks, {total_duration:.2f}s total, \"\n",
    "          f\"baseline μ={mu0[cell]:.3f}, σ={sd0[cell]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ba818",
   "metadata": {},
   "source": [
    "## Peak detection\n",
    "\n",
    "Transients are extracted from the robust $z$-scored traces using a thresholding strategy:\n",
    "\n",
    "* compute where the $z$-score exceeds `z_threshold` (default 3–4) indicating deviations beyond baseline noise;\n",
    "* enforce a refractory period (`min_distance` in seconds converted to samples) to avoid double-counting the same event;\n",
    "* optionally require a minimum prominence so small fluctuations are ignored.\n",
    "\n",
    "Because calcium dynamics are slow relative to the sampling rate, this simple detector performs surprisingly well. Nevertheless, revisit the parameters if you expect back-to-back spikes or very low signal-to-noise recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak detection parameters\n",
    "prominence_threshold = 3.0  # 3 SD threshold\n",
    "min_interval_sec = 0.7\n",
    "min_samples = max(1, int(round(min_interval_sec * fs)))\n",
    "NCELLS = 20  # Number of cells to plot\n",
    "\n",
    "# Detect peaks for all cells\n",
    "event_peaks = []\n",
    "for idx, z_trace in enumerate(z):\n",
    "    peaks, _ = find_peaks(z_trace, prominence=prominence_threshold, distance=min_samples)\n",
    "    event_peaks.append(peaks)\n",
    "\n",
    "# Create time axis\n",
    "time_axis = np.arange(z.shape[1]) / fs\n",
    "\n",
    "# Plot example traces\n",
    "fig, axes = plt.subplots(NCELLS, 1, figsize=(12, 3 * NCELLS), sharex=True)\n",
    "\n",
    "for i in range(NCELLS):\n",
    "    if i >= len(z):\n",
    "        break\n",
    "        \n",
    "    # Plot z-score trace\n",
    "    axes[i].plot(time_axis, z[i], 'b-', linewidth=1)\n",
    "    \n",
    "    # Mark detected peaks\n",
    "    peaks = event_peaks[i]\n",
    "    if len(peaks) > 0:\n",
    "        axes[i].scatter(time_axis[peaks], z[i, peaks], c='red', s=30, zorder=5)\n",
    "    \n",
    "    # Show threshold line\n",
    "    axes[i].axhline(y=prominence_threshold, color='orange', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(f'Cell {i}: {len(peaks)} peaks detected')\n",
    "    axes[i].set_ylabel('Z-score')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    if i == 4:  # Last subplot\n",
    "        axes[i].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "event_counts = [len(peaks) for peaks in event_peaks]\n",
    "print(f\"Peak detection summary (threshold = {prominence_threshold} SD):\")\n",
    "print(f\"Mean events per cell: {np.mean(event_counts):.1f}\")\n",
    "print(f\"Total events: {sum(event_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4559a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary event matrix for raster plot\n",
    "events_binary = np.zeros_like(z, dtype=int)\n",
    "for idx, peaks in enumerate(event_peaks):\n",
    "    if len(peaks) > 0:\n",
    "        events_binary[idx, peaks] = 1\n",
    "\n",
    "# Raster plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(events_binary, aspect='auto', cmap='Greys', interpolation='nearest')\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('Cell index')\n",
    "plt.title('Raster of detected events')\n",
    "plt.colorbar(label='Event')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a578e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "bin_size = 5  # seconds\n",
    "recording_duration = time_axis[-1]\n",
    "n_bins = int(np.ceil(recording_duration / bin_size))\n",
    "bin_edges = np.arange(0, n_bins * bin_size + bin_size, bin_size)\n",
    "\n",
    "# Sum events in each bin\n",
    "binned_activity = []\n",
    "for i in range(n_bins):\n",
    "    start_time = i * bin_size\n",
    "    end_time = (i + 1) * bin_size\n",
    "    start_idx = np.searchsorted(time_axis, start_time)\n",
    "    end_idx = np.searchsorted(time_axis, end_time)\n",
    "    bin_events = events_binary[:, start_idx:end_idx].sum()\n",
    "    binned_activity.append(bin_events)\n",
    "\n",
    "# Plot binned histogram\n",
    "bin_centers = bin_edges[:-1] + bin_size / 2\n",
    "plt.bar(bin_centers, binned_activity, width=bin_size*0.8, alpha=0.7, color='gray', edgecolor='black')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel(f'Number of events per {bin_size}s bin')\n",
    "plt.title(f'Population activity histogram ({bin_size}s bins)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "event_counts = [len(peaks) for peaks in event_peaks]\n",
    "print(f\"Peak detection summary (threshold = {prominence_threshold} SD):\")\n",
    "print(f\"Mean events per cell: {np.mean(event_counts):.1f}\")\n",
    "print(f\"Total events: {sum(event_counts)}\")\n",
    "print(f\"Mean events per {bin_size}s bin: {np.mean(binned_activity):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc877d0e",
   "metadata": {},
   "source": [
    "## Event frequency statistics\n",
    "\n",
    "Once peaks are labelled we can fold the detections into a per-cell summary table:\n",
    "\n",
    "* event counts, rates (events per minute), and z-scored amplitudes characterise how active each ROI is;\n",
    "* inter-event interval histograms separate tonic, Poisson-like firing from bursty cells;\n",
    "* global aggregates (e.g., fraction of silent cells) flag data-quality issues early.\n",
    "\n",
    "The next cell populates `summary_df`, a convenience DataFrame used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-cell event statistics ---\n",
    "recording_minutes = (n_time / fs) / 60 if fs else np.nan\n",
    "\n",
    "summary_records = []\n",
    "event_amplitudes = []\n",
    "event_iei = []\n",
    "for cell_idx, peaks in enumerate(event_peaks):\n",
    "    peaks = np.asarray(peaks, dtype=int)\n",
    "    z_trace = z[cell_idx]\n",
    "    amps = z_trace[peaks] if peaks.size else np.array([])\n",
    "    event_amplitudes.append(amps)\n",
    "\n",
    "    iei = np.diff(peaks) / fs if peaks.size > 1 and fs else np.array([])\n",
    "    if iei.size:\n",
    "        event_iei.append(iei)\n",
    "\n",
    "    rate = (len(peaks) / recording_minutes\n",
    "            if recording_minutes and np.isfinite(recording_minutes) and recording_minutes > 0\n",
    "            else np.nan)\n",
    "\n",
    "    summary_records.append({\n",
    "        'cell': cell_idx,\n",
    "        'event_count': int(len(peaks)),\n",
    "        'event_rate_per_min': float(rate),\n",
    "        'median_event_amp_z': float(np.nanmedian(amps)) if amps.size else np.nan,\n",
    "        'max_event_amp_z': float(np.nanmax(amps)) if amps.size else np.nan,\n",
    "        'median_iei_sec': float(np.nanmedian(iei)) if iei.size else np.nan,\n",
    "        'baseline_mean': float(mu0[cell_idx]),\n",
    "        'baseline_std': float(sd0[cell_idx]),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records).set_index('cell')\n",
    "\n",
    "print('Per-cell event metrics (first five rows):')\n",
    "display(summary_df.head())\n",
    "\n",
    "print('Descriptive statistics across the population:')\n",
    "display(summary_df.describe().T.round(3))\n",
    "\n",
    "global_event_summary = pd.Series({\n",
    "    'total_events': int(summary_df['event_count'].sum()),\n",
    "    'median_rate_events_per_min': float(summary_df['event_rate_per_min'].median()),\n",
    "    'fraction_silent_cells': float((summary_df['event_count'] == 0).mean()),\n",
    "})\n",
    "print('Global snapshot:')\n",
    "display(global_event_summary.to_frame('value'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "sns.histplot(summary_df['event_rate_per_min'].dropna(), bins=20, ax=axes[0], color='tab:purple')\n",
    "axes[0].set_xlabel('Event rate (events/min)')\n",
    "axes[0].set_ylabel('Number of cells')\n",
    "axes[0].set_title('Distribution of event rates')\n",
    "\n",
    "if any(amps.size for amps in event_amplitudes):\n",
    "    all_amplitudes = np.concatenate([amps for amps in event_amplitudes if amps.size])\n",
    "    sns.histplot(all_amplitudes, bins=30, ax=axes[1], color='tab:green')\n",
    "    axes[1].set_xlabel('Peak amplitude (z)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Event amplitude distribution')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No events detected', ha='center', va='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "if event_iei:\n",
    "    all_iei = np.concatenate(event_iei)\n",
    "    sns.histplot(all_iei, bins=30, ax=axes[2], color='tab:red')\n",
    "    axes[2].set_xlabel('Inter-event interval (s)')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].set_title('Inter-event interval distribution')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No inter-event intervals', ha='center', va='center', transform=axes[2].transAxes)\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2977ab",
   "metadata": {},
   "source": [
    "## Behavioural coupling to the continuous regressor\n",
    "\n",
    "With a per-cell event table in place we now quantify how neural activity aligns with the simultaneously recorded behaviour.\n",
    "We contrast linear (Pearson) and monotonic (Spearman) correlations to highlight both proportional and rank-order coupling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Behaviour alignment metrics ---\n",
    "behaviour = np.asarray(behaviour, dtype=float)[:n_time]\n",
    "behaviour_z = (behaviour - np.nanmean(behaviour)) / (np.nanstd(behaviour) + 1e-9)\n",
    "\n",
    "pearson_corr = []\n",
    "spearman_corr = []\n",
    "for cell_idx, z_trace in enumerate(z):\n",
    "    valid = np.isfinite(z_trace) & np.isfinite(behaviour_z)\n",
    "    if valid.sum() > 5:\n",
    "        pearson_val = pearsonr(z_trace[valid], behaviour_z[valid])[0]\n",
    "        spearman_val = spearmanr(z_trace[valid], behaviour_z[valid])[0]\n",
    "    else:\n",
    "        pearson_val = np.nan\n",
    "        spearman_val = np.nan\n",
    "    pearson_corr.append(pearson_val)\n",
    "    spearman_corr.append(spearman_val)\n",
    "\n",
    "summary_df['pearson_behaviour'] = pearson_corr\n",
    "summary_df['spearman_behaviour'] = spearman_corr\n",
    "\n",
    "print('Added Pearson and Spearman correlations to summary table.')\n",
    "display(summary_df[['event_rate_per_min', 'median_event_amp_z', 'pearson_behaviour', 'spearman_behaviour']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51da9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution of linear vs. monotonic coupling ---\n",
    "valid_pearson = summary_df['pearson_behaviour'].dropna()\n",
    "valid_spearman = summary_df['spearman_behaviour'].dropna()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "sns.histplot(valid_pearson, bins=20, ax=axes[0], color='tab:blue', alpha=0.8)\n",
    "axes[0].set_xlabel('Pearson r (linear)')\n",
    "axes[0].set_ylabel('Number of cells')\n",
    "axes[0].set_title('Distribution of linear coupling')\n",
    "\n",
    "sns.scatterplot(x=summary_df['pearson_behaviour'], y=summary_df['spearman_behaviour'], ax=axes[1], s=40, color='tab:green')\n",
    "axes[1].axline((0, 0), slope=1, color='0.4', linestyle='--', linewidth=1, label='Linear = monotonic')\n",
    "axes[1].set_xlabel('Pearson r')\n",
    "axes[1].set_ylabel('Spearman ρ')\n",
    "axes[1].set_title('Linear vs. monotonic coupling per cell')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cells analysed (Pearson): {len(valid_pearson)} / {n_cells}\")\n",
    "print(f\"Cells analysed (Spearman): {len(valid_spearman)} / {n_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d37e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example cells: positively and negatively coupled ---\n",
    "ranked = summary_df['pearson_behaviour'].dropna()\n",
    "if ranked.empty:\n",
    "    print('No valid behaviour correlations available.')\n",
    "else:\n",
    "    top_cell = int(ranked.idxmax())\n",
    "    bottom_cell = int(ranked.idxmin())\n",
    "    cells_to_show = [top_cell, bottom_cell]\n",
    "\n",
    "    fig, axes = plt.subplots(len(cells_to_show), 1, figsize=(14, 3.5 * len(cells_to_show)), sharex=True)\n",
    "    if len(cells_to_show) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, cell_idx in zip(axes, cells_to_show):\n",
    "        z_trace = z[cell_idx]\n",
    "        ax.plot(time_axis, z_trace, color='tab:blue', linewidth=1.0, label=f'Cell {cell_idx} (z)')\n",
    "        ax.set_ylabel('Activity (z)')\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(time_axis, behaviour_z, color='tab:orange', linewidth=1.0, alpha=0.6, label='Behaviour (z)')\n",
    "        ax2.set_ylabel('Behaviour (z)', color='tab:orange')\n",
    "\n",
    "        pearson_val = summary_df.loc[cell_idx, 'pearson_behaviour']\n",
    "        spearman_val = summary_df.loc[cell_idx, 'spearman_behaviour']\n",
    "        relation = 'positively' if pearson_val >= 0 else 'negatively'\n",
    "        ax.set_title(f\"Cell {cell_idx} {relation} coupled: Pearson = {pearson_val:.2f}, Spearman = {spearman_val:.2f}\")\n",
    "\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "    axes[-1].set_xlabel('Time (s)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dcfc2",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "You now have a reusable scaffold for baseline correction, transient detection, and coarse statistical characterisation of calcium imaging data. From here you can:\n",
    "\n",
    "* feed the $\\Delta F/F$ traces into dimensionality reduction or decoding pipelines (see the Hands-On PCA and ML notebooks);\n",
    "* swap the peak detector for deconvolution-based spike inference if higher temporal fidelity is required;\n",
    "* augment the statistics with condition averages, tuning curves, or cross-correlation analyses.\n",
    "\n",
    "As always, treat these first-order summaries as a diagnostic baseline before investing in heavier modelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SummerSchool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
